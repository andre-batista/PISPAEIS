\documentclass{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{subfigure}
\usepackage{xcolor}
\usepackage{epstopdf}

\graphicspath{{../}}
\epstopdfsetup{
    program@epstopdf=epstopdf
}

\newcommand{\brho}{\boldsymbol{\rho}}
\newcommand{\brhop}{\boldsymbol{\rho^\prime}}
\newcommand{\Es}{\mathbf{E^S}}
\newcommand{\GS}{\mathbf{G^S}}
\newcommand{\X}{\boldsymbol{\chi}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\Xo}{\boldsymbol{\bar{\chi}_o}}
\newcommand{\Xr}{\boldsymbol{\bar{\chi}_r}}

\begin{document}

    \title{To Be Decided}
    \author{Andr\'e Costa Batista, Ricardo Adriano, and Lucas S. Batista
        \thanks{This work has been supported by the Brazilian agencies Coordination for the Improvement of Higher Education Personnel - Brazil (CAPES) through the Academic Excellence Program (PROEX) under Grant 88887.815891/2023-00, FAPEMIG (Research Support Foundation of the State of Minas Gerais), and CNPq (The National Council for Scientific and Technological Development).}
        \thanks{A. C. Batista, R. Adriano, and L. S. Batista are with the Department of Electrical Engineering, Universidade Federal de Minas Gerais, Belo Horizonte, MG 31270-901, Brazil (e-mail: andre-costa@ufmg.br; rluiz@ufmg.br; lusoba@ufmg.br). A. C. Batista and L. S. Batista are also with Operations Research and Complex Systems Laboratory (ORCS Lab), Belo Horizonte, MG, Brazil.}}
   
    \maketitle
    
    \begin{abstract}
        To be written.
    \end{abstract}
    
    \begin{IEEEkeywords}
        Keyword 1, keyword 2, keyword 3. Remember: alphabetical order.
    \end{IEEEkeywords}
    
    \section{Introduction}\label{sec:introduction}

        % O Imageamento em Microondas é uma importante técnica para a detecção e caracterização de objetos em meios inacessíveis \cite{pastorino2010microwave}. Essa técnica de baixo custo utiliza ondas eletromagnéticas de baixa potência para obter informações sobre a estrutura interna de materiais de maneira não invasiva \cite{godinho2025evaluation}. As aplicações deste método são diversas, incluindo diagnósticos médicos \cite{mojabi2025microwave}, inspeção de estruturas \cite{fedeli2025mild}, through-wall imaging \cite{fedeli2020through}, sensoreamento remoto \cite{salucci2017multifrequency}, entre outros.

        \IEEEPARstart{M}{icrowave} Imaging is an important technique for the detection and characterization of objects in inaccessible media \cite{pastorino2010microwave}. This low-cost technique uses low-power electromagnetic waves to obtain information about the internal structure of materials in a non-invasive manner \cite{godinho2025evaluation}. The applications of this method are diverse, including medical diagnostics \cite{mojabi2025microwave}, structural inspection \cite{fedeli2025mild}, through-wall imaging \cite{fedeli2020through}, remote sensing \cite{salucci2017multifrequency}, among others.

        % Esta técnica é baseada na solução do problema inverso de espalhamento eletromagnético \cite{chen2018computational}. Ou seja, a partir das medições do campo eletromagnético fora do domínio de interesse, busca-se recuperar as propriedades do meio interno. Este problema é conhecido por ser mal posto, o que significa que pequenas variações nas medições podem levar a grandes variações na solução \cite{colton2019inverse}. Além disso, ele também é não-linear e não-convexo devido aos efeitos de espalhamento múltiplo \cite{chen2018computational}.

        This technique is based on solving the inverse electromagnetic scattering problem \cite{chen2018computational}. Specifically, electromagnetic field measurements collected outside the Domain of Interest (DOI) are used to recover the properties of the internal medium. This problem is inherently ill-posed, meaning that small variations in measurements can lead to large variations in the solution \cite{colton2019inverse}. Furthermore, the problem is both nonlinear and non-convex due to multiple scattering effects \cite{chen2018computational}. The problem can be formulated in either two or three dimensions, with two-dimensional formulations being more commonly used for algorithm development due to their computational simplicity and efficiency.
        
        % Quais classes de algoritmos e o que existe de mais novo.

        % Existe uma ampla gama de algoritmos para resolver o problema inverso de espalhamento eletromagnético. Estes algoritmos podem ser classificados em duas categorias principais: métodos qualitativos \cite{pastorino2010ch5}, que reconstroem a imagem sem estimar as propriedades elétricas do meio, e métodos quantitativos \cite{pastorino2010ch7}, os quais estimam essas propriedades. Entre os métodos qualitativos podemos destacar o Linear Sampling Method (LSM) \cite{chen2017lsm}, o Orthogonality Sampling Method (OSM) \cite{harris2020orthogonality}, entre outros. Entre os métodos quantitativos, podemos destacar o Born Iterative Method (BIM) \cite{wang1989iterative}, o Contrast Source Inversion (CSI) \cite{berg1997contrast}, o Distorted Born Inversion Method (DBIM) \cite{chew1990reconstruction}, o Subspace Optimization Method (SOM) \cite{chen2010subspace}, entre outros. Vale à pena destacar também que há algoritmos que contemplam certas especificidades do problema: como o método DORT \cite{chen2018computational} que aborda a reversão temporal dos sinais eletromagnéticos, o MUSIC \cite{zhong2007music}, o qual é adequado para pequenos espalhadores, o Compressive Sensing \cite{massa2015compressive}, o qual pode ser aplicado quando os dados de campo não tem fase, a Aproximação de Born de Primeira Ordem, o Backpropagation e a aproximação de Rytov \cite{chen2018computational}, os quais são úteis para problemas com espalhadores fracos. Outras abordagens relevantes são os Experimentos Virtuais \cite{isernia2024some,bevacqua2021simple}, os quais recombinam os dados do campo espalhado para explorar condições que aliviam a não-linearidade, e a reescrita eficiente das equações \cite{isernia2024some,bevacqua2021effective}, as quais tem o mesmo objetivo final de aliviar a não linearidade. Vale à pena destacar inclusive que, recentemente, algoritmos que fazem o uso de aprendizado de máquina tem sido propostos \cite{chen2020review}. Dentre as abordagens existentes, os métodos orientados a dados (data-driven), como o Direct Inversion Scheme \cite{yao2020enhanced,wei2019solving}, permitem a inferência direta dos parâmetros físicos de um espalhador a partir dos campos medidos, minimizando a necessidade de pré-processamento. Contudo, alguns métodos frequentemente demandam vastos conjuntos de dados para treinamento e apresentam notórias limitações em termos de generalização, confiabilidade e interpretabilidade. Para mitigar essas deficiências, foram propostos os métodos orientados à física (physics-driven), que incorporam leis e informações físicas para guiar o processo de reconstrução \cite{wei2019physics,wang2023push,hu2023more,liu2025exploring}. Outra abordagem é a construção de modelos substitutos para recuperação da posição, geometria e contraste dos espalhadores a partir de um problema de otimização \cite{salucci2022learned,zardi2025physics}. Apesar de superarem algumas das falhas das abordagens puramente baseadas em dados, sua eficácia pode estar condicionada a parâmetros do sistema, como a função de Green, cuja obtenção em meios complexos ou inomogêneos representa um desafio significativo.
        
        A wide range of algorithms exists for solving the inverse electromagnetic scattering problem. These algorithms can be broadly classified into two main categories: qualitative methods \cite{pastorino2010ch5}, which reconstruct images without estimating the electrical properties of the medium, and quantitative methods \cite{pastorino2010ch7}, which estimate these properties. 

        Among qualitative methods, notable examples include the Linear Sampling Method (LSM) \cite{chen2017lsm} and the Orthogonality Sampling Method (OSM) \cite{harris2020orthogonality}. Among quantitative methods, prominent approaches include the Born Iterative Method (BIM) \cite{wang1989iterative}, Contrast Source Inversion (CSI) \cite{berg1997contrast}, the Distorted Born Inversion Method (DBIM) \cite{chew1990reconstruction}, and the Subspace Optimization Method (SOM) \cite{chen2010subspace}.

        Specialized algorithms have been developed to address specific problem characteristics. These include the DORT method \cite{chen2018computational} for electromagnetic signal time reversal, MUSIC \cite{zhong2007music} for small scatterers, and Compressive Sensing \cite{massa2015compressive} for phaseless field data. For weak scatterer problems, effective approaches include First-Order Born Approximation, Backpropagation, and Rytov approximation \cite{chen2018computational}. 

        Additional relevant techniques include Virtual Experiments \cite{isernia2024some,bevacqua2021simple}, which recombine scattered field data to explore conditions that alleviate nonlinearity, and efficient equation reformulations \cite{isernia2024some,bevacqua2021effective} that share the same objective of reducing nonlinearity.

        Recently, machine learning-based algorithms have emerged as promising solutions \cite{chen2020review}. Among these approaches, data-driven methods such as the Direct Inversion Scheme \cite{yao2020enhanced,wei2019solving} enable direct inference of scatterer physical parameters from measured fields, minimizing preprocessing requirements. However, these methods often demand extensive training datasets and exhibit significant limitations in generalization, reliability, and interpretability. 

        To address these deficiencies, physics-driven methods have been proposed that incorporate physical laws and information to guide the reconstruction process \cite{wei2019physics,wang2023push,hu2023more,liu2025exploring}. Another approach involves constructing surrogate models for recovering scatterer position, geometry, and contrast through optimization problems \cite{salucci2022learned,zardi2025physics}. Despite overcoming some limitations of purely data-driven approaches, their effectiveness may depend on system parameters such as the Green's function, whose determination in complex or inhomogeneous media represents a significant challenge.

        % Quais formas de avaliar soluções e quais ferramentas já existem (incluir aquele trabalho do Kurrant que é software para câncer de mama).

        % Em grande parte destes trabalhos, os algoritmos propostos são testados e avaliados em métricas como o mean square error (MSE) da estimativa do contraste pixel a pixel ou algo similar \cite{yin2023subspace,zhang2022023,salucci2022learned,wang2023push,liu2022som,bevacqua2021simple,bevacqua2021effective}. Este tipo de abordagem leva em consideração a precisão na estimativa do contraste na forma direta e, indiretamente, pode dar uma perspectiva de quão boa a reconstrução da posição e geometria dos objetos pode ser, uma vez que erros na geometria e posicionamento podem afetar nessa métrica. No entanto, uma geometria bem precisa com uma má estimativa do contraste pode resultar em um erro considerável neste tipo de métrica, por exemplo. Logo, aspectos comparar o desempenho no aspecto de quão bem a geometria e a posição dos objetos são recuperadas não são contemplados adequadamente por este tipo de métrica. Além disso, a métrica MSE não é adequada para avaliar algoritmos qualitativos, uma vez que estes não estimam o contraste do meio. Vale à pena destacar também a métrica Structural Similarity (SSIM) \cite{wang2004image} to quantitatively show the reconstruction visual quality, which is designed to compute the mismatch between reconstructed contrast results and corresponding GTs both on the pixel-wise level and perceptual level. Ao invés de fazer uma comparação pixel a pixel, a métrica leva em consideração aspectos de luminância, contraste e estrutura de uma imagem qualquer. Portanto, ao mesmo tempo que fornece uma quantificação mais profunda da qualidade de construção como um todo, ela não vai mensurar diretamente aspectos isolados como geometria e posição.

        In most of these studies, the proposed algorithms are tested and evaluated using metrics such as the mean square error (MSE) of pixel-by-pixel contrast estimation or similar approaches \cite{yin2023subspace,zhang2023iterative,salucci2022learned,wang2023push,liu2022som,bevacqua2021simple,bevacqua2021effective}. This type of approach directly considers the precision of contrast estimation and can indirectly provide perspective on how well the reconstruction of object position and geometry might be, since errors in geometry and positioning can affect this metric. However, a highly accurate geometry with poor contrast estimation can result in considerable error in this type of metric, for example. Therefore, aspects of comparing performance regarding how well object geometry and position are recovered are not adequately addressed by this type of metric. Furthermore, the MSE metric is not suitable for evaluating qualitative algorithms, since these do not estimate the medium's contrast.

        It is also worth highlighting the Structural Similarity (SSIM) metric \cite{wang2004image} to quantitatively show the reconstruction visual quality, which is designed to compute the mismatch between reconstructed contrast results and corresponding ground truths both on the pixel-wise level and perceptual level \cite{wang2023push,liu2025exploring}. Instead of performing a pixel-by-pixel comparison, this metric considers aspects of luminance, contrast, and structure of any given image. Therefore, while it provides a deeper quantification of overall reconstruction quality, it does not directly measure isolated aspects such as geometry and position.

        Specifically for breast imaging, a set of metrics has been proposed in the literature that considers not only the precision in estimating electrical properties, but also the accuracy of the reconstructed tissue geometry \cite{kurrant2021evaluating}. Each geometry (original and reconstructed) is converted into vectors, and the metric is calculated using a formula that performs the dot product and norm of these vectors. To the best of the authors' knowledge, this is the only metric that can isolately consider the geometry aspect of the scatterer, even though it is designed specifically for breast imaging.

        % Descrever a lacuna para proposição de outros indicadores, tais como de posição e forma. Introduzir a minha proposta destacando vantagens, desvantagens e originalidade.

        % O objetivo deste trabalho é fornecer mais métricas para auxiliar na avaliação do desempenho de algoritmos. Com uma gama maior de indicadores pode ser possível ter uma visão mais abrangente sobre a capacidade dos algoritmos em diferentes cenários. Por isso, nossa contribuição se baseia na proposição de dois novos indicadores para avaliar o desempenho dos algoritmos na recuperação da geometria e posição dos espalhadores. O primeiro indicador quantifica o erro de forma, enquanto o segundo quantifica o erro de posição. A originalidade do trabalho reside no fato de que estes indicadores podem calcular de maneira mais isolada esses dois aspectos que são relevantes da descrição do desempenho de algoritmos para o problema inverso de espalhamento eletromagnético. Além disso, estes indicadores são projetados para serem aplicáveis a qualquer algoritmo que reconstrua a imagem, independentemente de estimar ou não o contraste.

        The objective of this work is to provide additional metrics to assist in evaluating algorithm performance. With a broader range of indicators, it becomes possible to obtain a more comprehensive understanding of algorithm capabilities across different scenarios. Therefore, our contribution is based on proposing two novel indicators for evaluating algorithm performance in recovering scatterer geometry and position. The first indicator quantifies shape error, while the second quantifies position error. 

        The originality of this work lies in the fact that these indicators can measure these two relevant aspects of electromagnetic inverse scattering algorithm performance in a more isolated manner. Furthermore, these indicators are designed to be applicable to any algorithm that reconstructs images, regardless of whether it estimates contrast or not.

        % Organização do texto

        The remainder of this paper is organized as follows. Section \ref{sec:problemstatement} presents the problem statement and the mathematical formulation of the electromagnetic inverse scattering problem. Section \ref{sec:indicators} introduces the proposed indicators for evaluating algorithm performance. Section \ref{sec:results} presents numerical results demonstrating the effectiveness of the proposed indicators. Finally, Section \ref{sec:conclusion} concludes the paper and discusses future work.
        
	\section{Problem Statement}\label{sec:problemstatement}
		
		Let $D \in \mathbb{R}^2$ denote the DOI embedded within a homogeneous, isotropic, nonmagnetic ($\mu = \mu_0 = 4\pi \times 10^{-7}$ H/m), and lossless ($\sigma = \sigma_0 = 0$ S/m) background medium with permittivity $\epsilon_b = \epsilon_{rb}\epsilon_0$, where $\epsilon_0 \approx 8.85 \times 10^{-12}$ F/m. We consider a 2D Transverse Magnetic (TMz) polarization where the DOI is illuminated by incident plane waves and assume the time convention $e^{j\omega t}$. All scatterers are located within the DOI, and the scattered field $E^s_z$ at point $\brho \in S$ outside $D$ is evaluated according to the following integral equation \cite{harrington2001time}:
		\begin{equation}
			E^s_z (\brho) = -\frac{k_b^2}{4} \int_D H_0^{(2)}(k_b|\brho-\brhop|)\chi(\brhop)E_z(\brhop) dS^\prime
		\end{equation}
		
		\noindent where $k_b = \omega\sqrt{\epsilon_b\mu_0} = 2\pi/\lambda_b$ is the background wave number, $\lambda_b$ is the wavelength of the incident wave, $H_0^{(2)}$ is the zero-order Hankel function of the second kind, $E_z$ is the total electric field in the DOI, and $\chi$ is the contrast function given by
		\begin{equation}
			\chi(\brho) = \frac{\epsilon_r(\brho)}{\epsilon_{rb}} - 1 - j\frac{\sigma(\brho)}{\omega\epsilon_b},
		\end{equation}
		
		\noindent which maps the relative permittivity $\epsilon_r(\brho)$ and conductivity $\sigma(\brho)$ distributions within $D$. It should be noted that the term $(-jk_b^2/4) H_0^{(2)}(k_b|\brho-\brhop|)$ corresponds to the Green's function in free space.
		
		In electromagnetic inverse scattering problems, we aim to recover the contrast function $\chi$ using a set of $N_M$ measurements of the scattered field collected for each of the $N_S$ sources. Additionally, $E_z$ is also unknown in $D$ and must be solved. For our formulation, the $N_S$ sources correspond to $N_S$ incidence angles of the plane wave, and the $N_M$ measurements are taken at $N_M$ points in $S$, arranged in a circular, equidistant array situated far from the center of the DOI by a radius $R_O$. By discretizing the DOI into $N_X \times N_Y = N$ pixels, the inverse problem is solved numerically according to the following equation:
		\begin{equation}
			E^s_{ms} = - \sum\limits_{n=1}^{N} G^S_{mn}\chi_{n}E_{ns} \label{eq:numerical}
		\end{equation}
		
		\noindent where $G^S$ is given according to the Richmond discretization \cite{richmond1965scattering, pastorino2010ch3}. Equation \eqref{eq:numerical} can also be expressed in matrix form:
		\begin{equation}
			\Es = \GS\X\E
		\end{equation}
		
		\noindent where $\Es$ is the $N_M \times N_S$ scattered field matrix, $\GS$ is the $N_M \times N$ Green's function matrix, $\X$ is the $N \times N$ contrast diagonal matrix, and $\E$ is the $N \times N_S$ total electric field matrix.
		
		It is important to highlight that, although assumptions are made regarding the scattered field domain and the incident field, the proposed indicators do not depend on these assumptions. They can be applied in any scenario where these entities are defined, and the assumptions in this paper serve only to clearly establish the context in which the indicators are tested.
	
	\section{Indicators}\label{sec:indicators}

		
		Once the result of the contrast function reconstruction is obtained by a given algorithm, the error in recovering the shape and location of the objects can be measured by comparing it with the original images. This means that the application of these metrics is relevant in studies where the exact response is known. Although in many real-world scenarios the imaged scatterer is unknown, applying these metrics to known cases can be valuable for comparing the performance of different algorithms and for estimating a confidence interval for the average performance of an algorithm in a given configuration. Therefore, these metrics are relevant tools that can contribute to the investigation and evaluation of algorithms for the problem.
		
		For the two indicators that will be explained below, the image of the original contrast function of the problem and the image reconstructed by any algorithm will be denoted by the matrices $\Xo$ and $\Xr$, both of size $N_X \times N_Y$. In other words, the elements of these matrices represent the pixels of the images.

		% Pressupostos:
		% 1. Função contraste (mapa) não seja uma função diferenciável (superfície).
		% 2. O algoritmo de reconstrução consegue discenir razoavelmente as bordas.
		% 3. No caso de múltiplos espalhadores com diferentes níveis de contraste, os menores níveis de constraste não estão abaixo da metade da diferença entre objeto de maior contraste e o fundo (**não ter contraste menor que zero**).
		% 4. Objetos de constraste diferentes não estejam sobrepostos (senão serão considerados como um objeto só).
	
		\subsection{Shape Error}\label{sec:indicators:shapeerror}
		
			% A função contraste tem valores complexos quando o meio ou os espalhadores possuem perdas. Portanto, o primeiro passo é tratar da possibilidade dos elementos das matrizes $\Xo$ e $\Xr$ terem partes reais e imaginárias. A abordagem mais simples é tomarmos o módulo das variáveis complexas como o valor de cada pixel. Esta escolha não elimina a possibilidade de que objetos com contrastes diferentes tenham o mesmo módulo. No entanto, nos casos onde somente existem materiais sem-perdas, esta abordagem não é problema porque o contraste é puramente real. Ao mesmo tempo, como o objetivo é verificar a forma dos espalhadores, o mais importante é fazer a separação entre meio-de-fundo e espalhador, sendo que o meio-de-fundo tem contraste zero. Portanto, esta escolha para simplificar a metodologia para mensurar o erro de forma não deve ter consequências relevantes em situações de verificação de desempenho do algoritmo.
			
			The contrast function has complex values when the medium or the scatterers have losses. Therefore, the first step is to account for the possibility that the elements of the matrices $\Xo$ and $\Xr$ have real and imaginary parts. The simplest approach is to use the magnitude of the complex variables as the value of each pixel. This choice does not eliminate the possibility that objects with different contrasts might have the same magnitude. However, in cases involving only lossless materials, this approach is not problematic because the contrast is purely real. Since the primary objective is to verify the shape of the scatterers, the critical aspect is distinguishing between the background medium and the scatterer, with the background medium having zero contrast. Thus, this approach to simplifying the methodology for measuring shape error should not have significant consequences in performance verification scenarios for the algorithm.
			
			% A função contraste reconstruída por muitos dos algoritmos -- tais como o BIM, DBIM, CSI, SOM, entre outros -- é comumente uma superfície contínua. Por isso, mesmo que a imagem original contenha apenas objetos de contorno bem definido e contraste homogêneo, a imagem destes objetos obtida pelos algoritmos possuem bordas suaves, i.e., o valor de contraste varia gradualmente entre o valor de meio-de-fundo e o valor do objeto. Por isso, identificar um contorno na imagem resultante é uma heurística porque é necessário escolher algum tipo de critério para definir a partir de que valor de contraste o pixel pode ser considerado como pertencente ao objeto. Por questões de simplicidade, o limiar $T$ para o qual vamos considerar que um píxel da imagem recuperada esteja dentro de um contorno de um objeto será definido como a metade da variação total do contraste na imagem, ou seja:
			
			The contrast function reconstructed by many algorithms -- such as BIM, DBIM, CSI, SOM, among others -- is commonly a discretized approximation of a continuous surface. Therefore, even if the original image contains only objects with well-defined boundaries and homogeneous contrast, the image of these objects obtained by the algorithms will have frequently smooth edges, i.e., the contrast value gradually varies between the background medium and the object's value. Consequently, identifying a contour in the resulting image is heuristic because it requires selecting a criterion to define the contrast value at which a pixel can be considered as belonging to the object. For simplicity, the threshold $T$ for considering a pixel in the recovered image to be within an object's contour will be defined as half of the total contrast variation in the image, i.e.:
			\begin{equation}
				T = \min(|\Xr|) + \frac{1}{2} \left[\max(|\Xr|)-\min(|\Xr|)\right]
			\end{equation}
		
			% Esta abordagem tem vantagens e desvantagens. Se a imagem reconstruída tiver múltiplos espalhadores no qual um ou mais deles possui um contraste abaixo desse limiar, a metodologia irá falhar em detectar o contorno deste ou destes objetos. No entanto, quando se trata de um caso onde há somente um espalhador na imagem -- o que é a maneira mais objetiva de se testar o potencial de um algoritmo em recuperar geometria, uma vez que a presença de múltiplos espalhadores pode deteriorar o desempenho de algoritmos por questões de influência das correntes induzidas em cada espalhador no espalhamento dos outros espalhadores -- é uma abordagem simples e objetiva e que não representa problemas para algoritmos que são capazes de captar bem os contornos, como métodos baseados aprendizado de máquina. Além disso, ao invés de utilizar os valores da imagem original, é mais interessante usar somente os valores da imagem reconstruída, tanto para isolar influência de erro na estimativa de contraste como para poder aplicar a métrica em métodos qualitativos que não fazem estimativa do contraste.
			
			This approach has advantages and disadvantages. If the reconstructed image contains multiple scatterers, and one or more of them has a contrast below this threshold, the methodology will fail to detect the contour of these objects. However, in scenarios where there is only one scatterer in the image, this approach is simple and effective. In fact, a single scatterer is the most objective way to test an algorithm's potential in recovering geometry, since the presence of multiple scatterers can degrade the algorithm's performance due to the influence of the induced currents in each scatterer on the scattering of the other. Furthermore, this approach poses no issues for algorithms capable of accurately capturing contours, such as machine learning-based methods. Moreover, instead of using the values from the original image, it is more beneficial to use only the values from the reconstructed image, both to isolate the influence of error in the contrast estimate and to apply the metric to qualitative methods that do not estimate contrast. If, for some reason, the original contrast function involves soft boundaries of the objects, the threshold might also be applied to the original image.
			
			%Uma vez aplicada a etapa de limiarização, os contornos dos espalhadores podem ser determinados. Uma das técnicas que podem ser aplicadas neste caso é o algoritmo Marching Squares \cite{lorensen1987marching}. Este algoritmo é capaz de retornar um conjunto de pontos que especificam cada contorno. Vale lembrar que objetos vazados possuem dois ou mais contornos. Além disso, se o algoritmo de imageamento falhar em detectar um objeto, haverá uma discrepância no número de contornos da imagem original e da reconstruída. Por essas razões, ao invés de comparar as geometrias através de um cálculo feito a partir dos pontos de cada contorno, nós preferimos realizar a comparação baseada na quantidade de pixeis que foram classificados incorretamente. Em outras palavras, após identificarmos quais pixeis estão dentro de cada contorno em cada imagem, podemos verificar quais deles faziam parte dos espalhadores na imagem original e não foram captados na imagem reconstruída (falso-negativo) e quais deles não faziam parte dos espalhadores na imagem original mas foram considerados na reconstruída (falso-positivo). Um dos questionamentos que pode surgir dessa escolha é por que não aplicar diretamente o cálculo de falsos-positivos e falsos-negativos após a limiarização. Uma das razões é que, normalmente, imagens originais e reconstruídas têm resoluções diferentes. Embora o método de interpolação pelo vizinho mais próxima seja uma alternativa para essa tarefa, a abordagem escolhida pode ser mais adequada para captar e preservar contornos de objetos principalmente quando a resolução da imagem reconstruída é bem inferior, conforme exemplificado na Fig. \ref{fig:contourmotivation}.
			
			Once the thresholding step is applied, the contours of the scatterers can be determined. One of the techniques suitable for this task is the Marching Squares algorithm \cite{lorensen1987marching}. This algorithm efficiently returns a set of points that delineate each contour. It is important to note that hollow objects may have two or more contours. Additionally, if the imaging algorithm fails to detect an object, a discrepancy will arise in the number of contours between the original and reconstructed images.
			
			For these reasons, instead of comparing the geometries through calculations based on the points of each contour, we prefer to perform the comparison based on the number of pixels that were misclassified. Specifically, after identifying which pixels fall within each contour in each image, we can determine which pixels belonged to the scatterers in the original image but were not captured in the reconstructed image (false negatives) and which pixels did not belong to the scatterers in the original image but were classified as such in the reconstructed image (false positives).
			
			A potential question that may arise from this choice is why not directly calculate false positives and false negatives immediately after thresholding. One reason is that original and reconstructed images typically have different resolutions. While the nearest neighbor interpolation method is an alternative for addressing this issue, the chosen approach may be more effective for capturing and preserving object contours, particularly when the resolution of the reconstructed image is significantly lower. This is illustrated in Fig. \ref{fig:contourmotivation}.
			
			\begin{figure}[!t]
				\centering
				\subfigure[]{\includegraphics[width=.49\columnwidth]{./figs/batis01}}
				\subfigure[]{\includegraphics[width=.49\columnwidth]{./figs/batis02}} \\
				\subfigure[]{\includegraphics[width=.49\columnwidth]{./figs/batis03}}
				\subfigure[]{\includegraphics[width=.49\columnwidth]{./figs/batis04}}
				\caption{An example illustrating the differences between nearest neighbor interpolation and the adopted contour detection methodology. (a) Original image of the scatterer (resolution 100x100); (b) Image reconstructed by an imaging algorithm after thresholding (20x20); (c) Image reconstructed to the original resolution using nearest neighbor interpolation; (d) Image reconstructed to the original resolution using contour identification.}
				\label{fig:contourmotivation}
			\end{figure}

            The sum of false negatives and false positives can be easily obtained by applying the XOR operation between the two images. Then, we can compute the shape error as the ratio between the number of incorrectly classified pixels and the total number of pixels that are part of the scatterers in the original image. Additionally, if we consider the shape error in percentage values, we can then define the indicator $\zeta_S$ as

            \begin{equation}
                \zeta_S = \frac{\text{FP} + \text{FN}}{\text{TP}} \times 100\%, \label{eq:shapeerror}
            \end{equation}

            \noindent where $\text{FP}$ and $\text{FN}$ are the numbers of false-positive and false-negative pixels, respectively, and $\text{TP}$ is the total number of pixels that are part of the scatterers in the original image. Therefore, the indicator assumes values greater than or equal to zero. Note that values greater than 100\% are also possible, indicating that the reconstructed image has more incorrectly classified pixels than the total number of pixels that make up the scatterers in the original image. This can occur more frequently when the number of pixels of the scatterer in the original image is small. The Algorithm \ref{alg:compute_zeta_s} summarizes the steps for calculating the shape error indicator.

            \input{algorithms/zetas.tex}
            			
		\subsection{Position Error}\label{sec:indicators:positionerror}

            % A metodologia para o cálculo do erro de detecção de posição possui similaridades. Da mesma forma como no cálculo de erro de forma, o cálculo do erro de posição também considera o valor absoluto do contraste das imagens e a mesma operação de limiarização. As razões para aplicar a mesma abordagem são as mesmas.

            The methodology for calculating the position detection error shares similarities with that of shape error calculation. As it is done when computing the shape error, the position error calculation also uses the absolute value of the image contrasts and applies the same thresholding operation. The reasons for this approach are the same.
            
            % A diferença é que o cálculo do erro de posição se baseia na comparação do ponto médio de pixeis classificados como objetos pelo operador de limiarização aplicado em cada imagem. Ou seja, para cada coordenada na imagem, é calculado a média dos valores de coordenada onde os objetos estão. Para isso, não é necessário aplicar o cálculo de contornos, uma vez que, mesmo tendo resoluções diferentes, os pontos das coordenadas estão dentro dos mesmos limites.

            The key difference lies in the method of position error calculation, which is based on comparing the centroid of pixels classified as objects by the thresholding operation in each image. Specifically, the centroid for each image is determined by averaging the coordinates of the pixels that constitute the objects. This method does not require contour detection, as the coordinate points are within the same boundaries for both images, even when resolutions differ.

            % Com base nos pontos médios em cada imagem, o indicador é definido como a distância euclidiana entre o par de pontos. Vale à pena destacar que os pontos são calculados em um intervalo de 0 a 1, onde 0 representa a origem da imagem e 1 representa o final da imagem. Desta forma, se multiplicarmos a distância por 100, podemos interpretar a distância como um erro percentual em relação ao tamanho da imagem. Em outra palavras, o indicador significa o quanto o algoritmo errou na localização dos objetos em relação ao tamanho da imagem. Isso permite também que o indicador seja usado em um conjunto de testes que tenham diferentes tamanhos de domínio.

            The position error indicator is then defined as the Euclidean distance between the centroids in the two images. It is important to note that these centroids are normalized within a range of 0 to 1, where 0 represents the image's origin and 1 represents its end. By multiplying this distance by 100, the error can be interpreted as a percentage relative to the image size. Thus, the indicator quantifies the error in object localization relative to the image size, allowing its application across test sets with varying domain sizes. Therefore, the position error indicator $\zeta_P$ is defined as:
            \begin{equation}
                \zeta_P = \sqrt{(x_c^R - x_c^O)^2 + (y_c^R-y_c^O)^2} \times 100\%, \label{eq:positionerror}
            \end{equation}

            \noindent where $(x_c^R, y_c^R)$ and $(x_c^O, y_c^O)$ are the centroids of the reconstructed and original images, respectively.

            % De fato, erros na recuperação da geometrias dos objetos podem influenciar diretamente no cálculo no erro de posição. Por exemplo, se um objeto no formato de uma estrela de cinco pontas fosse reconstruído com uma de suas pontas entortada, isso afetaria o ponto médio do objeto e, consequentemente, o erro de posição. No entanto, isso é esperado, uma vez que qualquer critério para considerar a posição de um objeto leva em conta os píxeis que o descrevem, assim como sua geometria. Por outro lado, o indicador proposto, além de ser simples, pode ser aplicado também em imagens com múltiplos objetos.

            It should be noted that inaccuracies in the reconstructed geometry of objects can significantly influence the position error calculation. For example, if a star-shaped object with five points is reconstructed with one point distorted, this distortion will affect the object's centroid, thereby impacting the position error. However, this effect is expected, as any criterion for determining an object's position inherently considers the pixels defining it, as it is for its geometry. Furthermore, the proposed indicator is straightforward and versatile, applicable even in scenarios involving images with multiple objects. The Algorithm \ref{alg:compute_zeta_p} summarizes the steps for calculating the position error indicator.

            \input{algorithms/zetap.tex}
	
	\section{Computational Experiments}\label{sec:results}
	
		% Os experimentos descritos neste artigo tem por objetivo ilustrar a aplicação dos indicadores propostos. Por isso, os experimentos foram desenhados para reforçar as possibilidades que existem com a introdução destes indicadores, e não necessariamente para obter resultados que representem novidades significativas na literatura. No entanto, as conclusões tiradas dos experimentos podem servir como base para outras investigações pela comunidade.

        The experiments presented in this article are intended to illustrate the application of the proposed indicators. The design of these experiments focuses on demonstrating the potential utility of these indicators, rather than achieving pioneering results in the field. Nevertheless, the findings from these experiments can still provide a foundation for further research by the scientific community.

        % Os experimentos foram realizados por meio da biblioteca de código-aberto EISPY2D \cite{batista2022eispy2d}, a qual foi construída especificamente para desenhar e medir o desempenho de algoritmos para o problema inverso de espalhamento eletromagnético. Ambos indicadores serão abordados e, para cada abordagem, foram realizados dois estudos de caso (um básico e outro explorando alguma particularidade) e um estudo de benchmarking. Os estudos de caso tem por objetivo medir o desempenho em situações específicas, enquanto o estudo de benchmarking permite uma maior generalização dos resultados.

        The experiments were conducted using the open-source library EISPY2D \cite{batista2025eispy2d}, specifically developed to design and evaluate algorithms for the inverse electromagnetic scattering problem. Both indicators will be analyzed, with each approach featuring two case studies (one fundamental and the other exploring a particular aspect) as well as a benchmarking study. The case studies aim to assess performance in specific scenarios, while the benchmarking study facilitates broader generalization of the results. Additionally, an experiment considering a breast imaging scenario was conducted to illustrate the applicability of the proposed indicators in a realistic scenario.

        % For all experiments, the following common parameters were used: $\lambda_b$ = 1 m, noise level = 5\%, relative permittivity of the background medium $\epsilon_{rb}$ = 1, and incident wave amplitude = 1 V/m. Data synthesis was performed using the MoM-CG-FFT algorithm \cite{su1987calculation} with 5000 iterations. A total of 80 measurement points ($N_M$) and 80 incidence angles ($N_S$) were used. The dimensions of the DoI were set to $2\lambda_b\times2\lambda_b$, and the measurement points' distance radius $R_O$ was fixed at $4\lambda_b$. All reconstructions were performed using a discretization of 40$\times$40 pixels.

        For all experiments in Section \ref{sec:results:shape} and \ref{sec:results:position}, the following common parameters were used: background wavelength $\lambda_b = 1$ m, noise level of 5\%, relative permittivity of the background medium $\epsilon_{rb} = 1$, and incident wave amplitude of 1 V/m. 

        Data synthesis was performed using the MoM-CG-FFT algorithm \cite{su1987calculation} with 5000 iterations. The measurement configuration consisted of 80 measurement points ($N_M$) and 80 incidence angles ($N_S$), with measurement points arranged in a circular array at distance $R_O = 4\lambda_b$ from the DOI center. 

        The DOI dimensions were set to $2\lambda_b \times 2\lambda_b$, and all reconstructions were performed using a $40 \times 40$ pixel discretization.

		\subsection{Shape Recovering Study}\label{sec:results:shape}
		
			% Para estudar o erro de forma, dois estudos de caso foram realizados. O primeiro estudo de caso consiste em um único espalhador com contraste homogêneo. O segundo estudo de caso envolve um espalhador com contraste variável. Ambos estudos de caso foram escolhidos para ilustrar a aplicação do indicador proposto em cenários comuns em problemas de espalhamento eletromagnético. O estudo de benchmarking foi realizado para medir o desempenho médio dos algoritmos em situações mais gerais.

            To evaluate shape error, two case studies were conducted. The first case study involves a single scatterer with homogeneous contrast, while the second case study involves a scatterer with variable contrast. These case studies were selected to demonstrate the application of the proposed indicator in typical scenarios encountered in electromagnetic scattering problems. Additionally, a benchmarking study was performed to assess the average performance of algorithms in more general situations.
			
            % Em todos os casos, as dimensões do DoI foram ajustadas para $2\lambda_b\times2\lambda_b$. O raio de distância dos pontos de medição $R_O$ foi fixado em 4$\lambda_b$.

			\subsubsection{Single scatterer}\label{sec:results:shape:star}

                % O primeiro estudo de caso foi formulado com objetivo de verificar a aplicabilidade do indicador em um cenário simples, onde diferentes algoritmos pudessem ser aplicados. O estudo considera um espelhador com formato de uma estrela de cinco pontas posicionada no centro da imagem. Esta escolha foi feita com o objetivo de escolher uma geometria que, ao mesmo tempo que é simétrica e bem definida, possui um número maior de vértices que geometria mais comuns, como quadrados ou triângulos. O contraste foi fixado em 0.25 e o raio do centro do espalhador até os vértices mais distantes foi fixado em 0.9$\lambda_b$. Foram considerados 80 pontos de medição ($N_M$) e 80 ângulos de incidência ($N_S$). Nessas condições, o Grau de Não Linearidade (DNL) do teste vale 0.12, o qual é significativamente menor que o limiar em que a Aproximação de Born deixa de ser aplicável, i.e., 1 \cite{bucci2001degree}. O campo espalhado foi sintetizado com base numa discretização de 120$\times$120 pixels da imagem original e todas as reconstruções foram feitas com base numa discretização de 40$\times$40 pixels.

                The first case study was designed to evaluate the applicability of the proposed indicator in a straightforward scenario where various algorithms could be applied. This case study features a scatterer shaped like a five-pointed star, centrally positioned within the image. This choice was made to select a geometry that, while symmetric and well-defined, has more vertices compared to more common shapes such as squares or triangles. The contrast was set to 0.25, and the radius from the center of the scatterer to the farthest vertices was set to 0.9$\lambda_b$. Under these conditions, the Degree of Nonlinearity (DNL) for the test is 0.12, which is significantly lower than the threshold beyond which the Born Approximation becomes inapplicable, i.e., 1 \cite{bucci2001degree}. The scattered field was synthesized using a discretization of 120$\times$120 pixels for the original image.

                % Os seguintes algoritmos e suas repectivas configurações foram considerados neste estudo:
                % \begin{itemize}
                %     \item Linear Sampling Method (LSM):
                %     \begin{itemize}
                %         \item Limiar: 0.7;
                %         \item Método de Regularização: Gradiente Conjugado com 300 iterações.
                %     \end{itemize}
                %     \item Orthogonality Sampling Method (OSM):
                %     \begin{itemize}
                %         \item Limiar: 0.35;
                %     \end{itemize}
                %     \item Born Iterative Method (BIM):
                %     \begin{itemize}
                %         \item Método de Regularização: Gradiente Conjugado com 300 iterações.
                %         \item Critério de Parada: 30 iterações.
                %     \end{itemize}
                %     \item Contrast Source Inversion (CSI):
                %     \begin{itemize}
                %         \item Critério de Parada: 300 iterações.
                %     \end{itemize}
                %     \item Subspace Optimization Method (SOM):
                %     \begin{itemize}
                %         \item Critério de Parada: 30 iterações.
                %         \item Índice de corte de autovalores: 5.
                %     \end{itemize}
                % \end{itemize}

                For this experiment, the following algorithms were considered: LSM, OSM, BIM, CSI, and SOM. Their parameters were chosen after empirical tests that aimed to achieve their best performance. The configurations for each algorithm are as follows:
                \begin{itemize}
                    \item Linear Sampling Method (LSM):
                    \begin{itemize}
                        \item Threshold: 0.7;
                        \item Regularization Method: Conjugate Gradient with 300 iterations.
                    \end{itemize}
                    \item Orthogonality Sampling Method (OSM):
                    \begin{itemize}
                        \item Threshold: 0.35;
                    \end{itemize}
                    \item Born Iterative Method (BIM):
                    \begin{itemize}
                        \item Regularization Method: Conjugate Gradient with 300 iterations.
                        \item Stopping Criterion: 30 iterations.
                    \end{itemize}
                    \item Contrast Source Inversion (CSI):
                    \begin{itemize}
                        \item Stopping Criterion: 300 iterations.
                    \end{itemize}
                    \item Subspace Optimization Method (SOM):
                    \begin{itemize}
                        \item Stopping Criterion: 30 iterations.
                        \item Eigenvalue Cutoff Index: 5.
                    \end{itemize}
                \end{itemize}

                The images reconstructed by each algorithm are shown in Fig. \ref{fig:shape:single:recons}. For the qualitative methods, OSM exhibited contrast variation within the object, which is consistent with the inherent characteristics of the method. Additionally, OSM recovered a larger area with more defined vertices compared to LSM. Regarding the quantitative methods, CSI demonstrated the lowest performance in recovering the object's contrast value close to the ground truth. Moreover, the quantitative methods produced more accurate contours than their qualitative counterparts.

                \begin{figure*}[!htb]
                    \subfigure[Ground-Truth]{\includegraphics[width=0.33\textwidth]{./figs/shape/single/recons/0}}
                    \subfigure[LSM]{\includegraphics[width=0.33\textwidth]{./figs/shape/single/recons/1}}
                    \subfigure[OSM]{\includegraphics[width=0.33\textwidth]{./figs/shape/single/recons/2}} \\
                    \subfigure[BIM]{\includegraphics[width=0.33\textwidth]{./figs/shape/single/recons/3}}
                    \subfigure[CSI]{\includegraphics[width=0.33\textwidth]{./figs/shape/single/recons/4}}
                    \subfigure[SOM]{\includegraphics[width=0.33\textwidth]{./figs/shape/single/recons/5}}
                    \caption{Shape recovering study, single scatterer: ground-truth and reconstructed images by each algorithm.}
                    \label{fig:shape:single:recons}
                \end{figure*}
		
                Table \ref{tab:star} summarizes the results of the shape error indicator for each algorithm. The results indicate that BIM and SOM achieved the best performance, while CSI, LSM and OSM performed similarly. This is consistent with the observations from Fig. \ref{fig:shape:single:recons}, since the quantitative methods had a greater ability to capture the object's contours compared to the qualitative methods. This was not true for CSI, which, due to its greater difficulty in quantifying the object's contrast, had more difficulty in capturing the object's contours.           

                \begin{table}[!htb]
                    \centering
                    \renewcommand{\arraystretch}{1.5}
                    \caption{Shape recovering study, single scatterer: shape error indicator $\zeta_S$ for each algorithm.}
                    \label{tab:star}
                    \begin{tabular}{cccccc}
                        Method & LSM & OSM & BIM & CSI & SOM \\\hline
                        $\zeta_S$ (\%) & 29.2 & 24.5 & 14.9 & 26.3 & 15.4 \\
                    \end{tabular}   
                \end{table}
			
			\subsubsection{Varying contrast}\label{sec:results:shape:varying}

               % Um outro tipo de estudo possível que pode complementar a análise do desempenho dos algoritmos na reconstrução da geometria dos objetos é a observação do desempenho do indicador com a variação do contraste de um objeto. Ou seja, dado um espalhador, a medida que o seu valor de contraste aumenta, o DNL aumenta e, consequentemente, os algoritmos podem peder desempenho uma vez que o problema fica mais díficil. Para demonstrar isso, o mesmo espalhador com as mesmas configurações de cenário do estudo anterior foi considerado. A única excepção na configuração foi que a resolução da imagem original foi aumentada para 160$\times$160 pixels por questões de precisão no cálculo do campo. O espalhador foi reconstruído com base em cinco valores de contraste: 0.25, 0.5, 0.75, 1 e 1.5. A Fig. \ref{fig:shape:vary:dnl} mostra o DNL para cada valor de contraste. Para este estudo foram considerados os métodos OSM e SOM com as mesmas configurações do estudo anterior. Obviamente, os parâmetros dos métodos poderiam ser ajustados para cada valor de contraste. No entanto, como o experimento tem apenas o fim de demonstrar a aplicabilidade do indicador, os parâmetros foram mantidos constantes para todos os valores de contraste.

                Another type of study that can complement the analysis of algorithm performance in object geometry reconstruction is the observation of indicator performance with varying object contrast. Specifically, for a given scatterer, as its contrast value increases, the DNL increases, and consequently, algorithms may lose performance since the problem becomes more challenging. To demonstrate this relationship, the same scatterer from the previous study was considered with identical scenario configurations. The only modification was that the original image resolution was increased to 160$\times$160 pixels to ensure precision in field calculations for high contrast scenarios. 

                The scatterer was reconstructed using five contrast values: 0.25, 0.5, 0.75, 1.0, and 1.5. Fig. \ref{fig:shape:vary:dnl} shows the corresponding DNL for each contrast value. For this study, the OSM and SOM methods were employed with the same configurations as the previous study. While the method parameters could be optimized for each contrast value, they were kept constant across all contrast values to demonstrate the indicator's applicability in a controlled manner.

                \begin{figure*}[!htb]
                    \centering
                    \subfigure[OSM, $\chi=0.25$]{\includegraphics[height=1.2in]{./experiments/vary/figs/osm_0.eps}}
                    \subfigure[OSM, $\chi=0.5$]{\includegraphics[height=1.2in]{./experiments/vary/figs/osm_1.eps}}
                    \subfigure[OSM, $\chi=0.75$]{\includegraphics[height=1.2in]{./experiments/vary/figs/osm_2.eps}}
                    \subfigure[OSM, $\chi=1$]{\includegraphics[height=1.2in]{./experiments/vary/figs/osm_3.eps}}
                    \subfigure[OSM, $\chi=1.5$]{\includegraphics[height=1.2in]{./experiments/vary/figs/osm_4.eps}} \\
                    \subfigure[SOM, $\chi=0.25$]{\includegraphics[height=1.in]{./experiments/vary/figs/som_0.eps}}
                    \subfigure[SOM, $\chi=0.5$]{\includegraphics[height=1.in]{./experiments/vary/figs/som_1.eps}}
                    \subfigure[SOM, $\chi=0.75$]{\includegraphics[height=1.in]{./experiments/vary/figs/som_2.eps}}
                    \subfigure[SOM, $\chi=1$]{\includegraphics[height=1.in]{./experiments/vary/figs/som_3.eps}}
                    \subfigure[SOM, $\chi=1.5$]{\includegraphics[height=1.in]{./experiments/vary/figs/som_4.eps}}
                    \caption{Shape recovering study, varying contrast: reconstructed images by OSM and SOM for different contrast values.}
                    \label{fig:shape:vary:recons}
                \end{figure*}
                
                % A Fig. \ref{fig:shape:vary:recons} mostra as imagens reconstruídas por OSM e SOM para cada valor de contraste. Conforme esperado, a medida que o contraste aumentou, a precisão na recuperação da geometria diminuiu, ao ponto de que, para o maior valor de contraste, 1.5, a geometria não foi mais recuperada corretamente, principalmente no caso do SOM. É possível observar que o SOM conseguiu estimar o contraste do espalhador com algum grau de precisão até o caso onde $\chi = 1$, no qual o DNL é 2, aproximadamente.

                Fig. \ref{fig:shape:vary:recons} shows the images reconstructed by OSM and SOM for each contrast value. As expected, reconstruction precision decreased with increasing contrast, reaching a point where the geometry was no longer correctly recovered for the highest contrast value (1.5), particularly in the case of SOM. Notably, SOM was able to estimate the scatterer's contrast with reasonable precision up to $\chi = 1$, where the DNL is approximately 2.

                % Fig. \ref{fig:shape:vary:indicators:zeta_s} shows the shape error indicator $\zeta_S$ for OSM and SOM. The results indicate that the shape error increases with the contrast value, which is consistent with the expected behavior of the algorithms. Além disso, no caso $\chi = 1.5$, o indicador $\zeta_S$ assume valores maiores que 100\%, indicando que o número de pixeis classificados incorretamente é maior que o número de pixeis que compõem o espalhador na imagem original. Isso é consistente com a observação de que, para este valor de contraste, a geometria não foi mais recuperada corretamente. Observa-se também que a diferença entre os indicadores de OSM e SOM se mantém relativamente constante até o caso $\chi = 1$.

                Fig. \ref{fig:shape:vary:indicators:zeta_s} presents the shape error indicator $\zeta_S$ for both OSM and SOM methods. The results demonstrate that the shape error increases proportionally with contrast value, which is consistent with the expected algorithmic behavior. Furthermore, for $\chi = 1.5$, the indicator $\zeta_S$ exceeds 100\%, indicating that the number of incorrectly classified pixels surpasses the number of pixels composing the scatterer in the original image. This observation aligns with the finding that geometry recovery fails at this contrast level. Additionally, the difference between the OSM and SOM indicators remains relatively constant up to $\chi = 1$.

                \begin{figure}[!htb]
                    \centering
                    \subfigure[]{\includegraphics[width=0.48\columnwidth]{./experiments/vary/figs/zeta_s.eps}\label{fig:shape:vary:indicators:zeta_s}} \hspace{0.01\columnwidth}
                    \subfigure[]{\includegraphics[width=0.48\columnwidth]{./experiments/vary/figs/dnl.eps}\label{fig:shape:vary:dnl}}
                    \caption{Shape recovering study, varying contrast: (a) shape error indicator $\zeta_S$ for OSM and SOM; (b) Degree of Nonlinearity (DNL) for each contrast value.}
                    \label{fig:shape:vary:indicators}
                \end{figure}

			\subsubsection{Average performance}\label{sec:results:shape:average}

                % Quando dois ou mais algoritmos vão ser comparados, é interessante que o estudo de caso seja feito com base em um conjunto de testes que reflitam uma classe específica do problema. Com base nos resultados, o desempenho médio dos algoritmos pode ser medido e comparado com rigor estatítico. Isto dá suporte a conclusões mais robustas sobre o desempenho dos algoritmos.

                When comparing multiple algorithms, it is advantageous to conduct case studies using test sets that represent specific problem classes. This approach enables the measurement and statistical comparison of average algorithm performance, thereby supporting more robust and reliable conclusions regarding their relative effectiveness.
                
                % Desta forma, foi conduzido um experimento para comparar o desempenho dos algoritmos LSM, OSM, BIM, CSI e SOM no indicador de erro de forma proposto. Para este experimento, foram considerados 30 imagens de espalhadores com formatos variados. O formato desses espalhadores foi gerado aleatoriamente como polígonos com 4 a 15 vértices, com um raio de distância dos vértices mais distantes do centro do espalhador variando entre 0.45$\lambda_b$ e 0.9$\lambda_b$. O contraste dos espalhadores foi fixado em 0.25 e os mesmos parâmetros para sintetização dos dados utilizados no estudo da subsubseção \ref{sec:results:shape:star} foram utilizados. Vale a pena destacar que, uma vez que as geometrias variam, o DNL de cada instância pode variar, conforme mostrado na Fig. \ref{fig:shape:average:dnl}. Conforme é possível observar, a mediana do DNL está acima do valor no experimento da subsubseção \ref{sec:results:shape:star}. No entanto, as duas são significativamente próximas, principalmente quando comparadas ao limiar de 1, que é o valor a partir do qual a Aproximação de Born deixa de ser aplicável \cite{bucci2001degree}. Desta forma, o objetivo do experimento é buscar evidências estatísticas que suportem a superioridade de algum dos algoritmos para essa classe de problemas.

                Following such an approach, an experiment was conducted to compare the LSM, OSM, BIM, CSI, and SOM algorithms. This experiment utilized 30 scatterer images with varying geometries to provide a more comprehensive assessment of algorithm performance across different shape configurations.

                The scatterers were randomly generated as polygons with 4 to 15 vertices, where the radius from the center to the farthest vertices ranged between 0.45$\lambda_b$ and 0.9$\lambda_b$. The contrast was fixed at 0.25 for all scatterers, and the data synthesis parameters from Section \ref{sec:results:shape:star} were maintained for consistency. 

                Due to the geometric variations, the DNL for each instance varies accordingly, as illustrated in Fig. \ref{fig:shape:average:dnl}. The median DNL exceeds the value from the experiment in Section \ref{sec:results:shape:star}, though both remain significantly below the threshold of 1, above which the Born Approximation becomes inapplicable \cite{bucci2001degree}. This experimental design enables the assessment of statistical evidence supporting the superiority of any algorithm within this problem class.

                \begin{figure}[!htb]
                    \centering
                    \subfigure[]{\includegraphics[width=.5\columnwidth]{./experiments/average/figs/dnl.eps}\label{fig:shape:average:dnl}} \\
                    \subfigure[]{\includegraphics[width=\columnwidth]{./experiments/average/figs/plot.eps}\label{fig:shape:average:zeta_s}}
                    \caption{Shape recovering study, average performance: (a) Degree of Nonlinearity (DNL) for each scatterer; (b) shape error indicator $\zeta_S$ for each algorithm in each instance.}
                    \label{fig:shape:average}   
                \end{figure}

                % Fig. \ref{fig:shape:average:zeta_s} shows the shape error indicator $\zeta_S$ for each algorithm across all instances. É possível observar uma clara diferença entre o LSM e os demais algoritmos, com o LSM apresentando o pior desempenho. Os outros métodos possuem desempenhos similares, embora seja possível observar que o OSM possuiu um erro maior em um número significativo de instâncias e que o CSI, por sua vez, teve um desempenho ligeiramente melhor em um número significativo de instâncias.

                Fig. \ref{fig:shape:average:zeta_s} presents the shape error indicator $\zeta_S$ for each algorithm across all test instances. The results reveal a clear performance distinction when considering LSM, which demonstrates significantly worse performance compared to the other methods. Among the remaining algorithms, while overall performance levels appear similar, some patterns emerge: CSI exhibits higher error rates across a significant number of instances, whereas BIM demonstrates superior performance in a significant portion of the test cases.

                % Enquanto a análise da Fig. \ref{fig:shape:average:zeta_s} oferece as primeiras impressões sobre os resultados, a aplicação de testes estatísticos é necessária para validar as conclusões. Para isso, o teste de Friedman foi aplicado para verificar se existe uma diferença significativa entre os algoritmos. O teste de Friedman é um teste não paramétrico que avalia diferenças pareadas significativas entre três ou mais grupos relacionados. Neste caso, os grupos são os algoritmos e as medições são os valores do indicador $\zeta_S$ para cada algoritmo em cada instância. Uma vez que o LSM apresentou um desempenho significativamente pior que os demais algoritmos, ele foi removido da análise estatística.

                While Fig. \ref{fig:shape:average:zeta_s} provides initial insights into the results, statistical testing is important to validate the conclusions rigorously. To this end, Randomized Complete Block Design (RCBD) was applied to assess whether significant paired differences exist among the algorithms. RCBD is a statistical test which is used to evaluate significant paired differences between three or more related groups when the data meet the assumptions for parametric tests. In this context, the groups correspond to the algorithms, and the measurements are the $\zeta_S$ indicator values for each algorithm across all instances. Given that LSM demonstrated substantially inferior performance compared to the other algorithms, it was excluded from the statistical analysis to focus on the more competitive methods.

                % O resultado do teste de Friedman foi $p < 10^{-10}$, indicando que existe pelo menos um par de algoritmos com uma diferença significativa. Em seguida, foi aplicado o teste de Wilcoxon para identificar quais algoritmos possuem diferenças significativas entre si. Os valores-p obtidos pelos testes são apresentados na Tabela \ref{tab:shape:average:pvalue}. Conforme é indicado na tabela, a hipótese nula de que os algoritmos possuem desempenhos iguais é rejeitada para todos os pares de algoritmos.

                After validating normality assumption, the p-value computed by RCBD result was less than $10^{-15}$, indicating that at least one pair of algorithms shows a significant mean paired difference. Subsequently, multiple paired T-Test with Bonferroni correction were applied to identify which algorithms have significant mean paired differences between them. The p-values obtained from these tests are presented in Table \ref{tab:shape:average:pvalue}. As shown in the table, the null hypothesis that algorithms have equal performance is rejected for all algorithm pairs, except for OSM-SOM, indicating that OSM and SOM do not exhibit a significant difference in their average performance.

                \begin{table*}
                    \centering
                    \renewcommand{\arraystretch}{1.5}
                    \caption{Shape recovering study, average performance: $p$-values for the paired T-Test with Bonferroni correction.}
                    \label{tab:shape:average:pvalue}
                    \begin{tabular}{ccccccc}
                        Pairs & OSM-BIM & OSM-CSI & OSM-SOM & BIM-CSI & BIM-SOM & CSI-SOM \\\hline
                        $p$-value & $<10^{-2}$ & $<10^{-11}$ & 0.15 & $<10^{-7}$ & $<10^{-4}$ & $<10^{-6}$ \\  
                    \end{tabular}
                \end{table*}

                % Para complementar a análise, a Fig. \ref{fig:shape:average:confidence_intervals} apresenta os intervalos de confiança (95\%) para a média das diferenças pareadas do indicador de erro de forma $\zeta_S$ para cada par de algoritmos. Em todos os casos, foi necessária alguma forma de transformação dos dados (logarítmica, raiz quadrada ou Box-Cox) para que os dados seguissem uma distribuição normal. No entanto, no gráfico, os dados foram transformados de volta para a escala original. Como é possível observar, a média das diferenças sempre favorece o CSI, o que significa que o teste fornece evidência para a superiodidade do CSI em relação aos demais algoritmos. Além disso, o OSM é significativamente pior que o BIM e o SOM, enquanto o BIM possui desempenho melhor que o SOM. No entanto, vale à pena destacar que, essas diferenças, na média, não são maiores que 5\%. Por fim, vale à pena observar que, enquanto no estudo da seção \ref{sec:results:shape:star} o CSI teve um desempenho pior que o BIM e o SOM, neste estudo de benchmarking, o CSI teve um desempenho melhor que ambos. Isso reforça a importância de realizar estudos de benchmarking combinados com análises estatísticas para obter conclusões mais robustas sobre o desempenho dos algoritmos.

                To complement the analysis, Fig. \ref{fig:shape:average:confidence_intervals} presents the 95\% confidence intervals for the mean paired differences of the shape error indicator $\zeta_S$ for each algorithm pair. As observed, the differences consistently favor BIM, providing evidence for BIM's superiority over the other algorithms. Additionally, CSI performs significantly worse than both BIM, OSM and SOM. However, all differences in average performance are less than 7\%.

                Furthermore, it is noteworthy that BIM also achieved the best performance in the study from Section \ref{sec:results:shape:star}. However, while the difference between SOM and OSM was significant in the experiment from Section \ref{sec:results:shape:star}, this benchmarking study provided evidence for performance equivalence between these two methods. In other words, if conclusions were drawn based solely on the case study, this could lead to a misconception about the performance of these two methods in this problem configuration. Therefore, this reinforces the importance of conducting benchmarking studies combined with statistical analyses to obtain more robust conclusions about algorithm performance.

                \begin{figure}
                    \centering
                    \includegraphics[width=.8\columnwidth]{./experiments/average/figs/confidence_intervals.eps}
                    \caption{Shape recovering study, average performance: Confidence intervals (95\%) for the shape error indicator $\zeta_S$ for each pair of algorithms.}
                    \label{fig:shape:average:confidence_intervals}
                \end{figure}

                % Com base nos resultados, foi possível aplicar o teste de Friedman \cite{friedman1937use} para verificar se existe uma diferença significativa entre os algoritmos. O teste de Friedman é um teste não paramétrico que verifica se existe uma diferença significativa entre três ou mais grupos relacionados. Neste caso, os grupos são os algoritmos e as medições são os valores do indicador $\zeta_S$ para cada algoritmo em cada instância. O resultado do teste de Friedman foi $p = 0.0001$, indicando que existe uma diferença significativa entre os algoritmos. Em seguida, foi aplicado o teste de Nemenyi \cite{nemenyi1963distribution} para verificar quais algoritmos possuem diferenças significativas entre si. O resultado do teste de Nemenyi indicou que o LSM é significativamente pior que todos os outros algoritmos, enquanto os demais métodos não possuem diferenças significativas entre si.
		
		\subsection{Position Detection Study}\label{sec:results:position}

            % Em muitas configurações nas quais os algoritmos conseguem reconstruir adequadamene a geometria dos espalhadores, o erro na detecção da posição dos espalhadores tende a ser baixo. Por exemplo, no estudo da seção \ref{sec:results:shape:star}, o indicador $\zeta_P$ obtido pelo LSM, OSM, BIM, CSI e SOM foram, respectivamente, 0.049\%, 0.057\%, 0.25\%, 0.31\% e 0.14\%. Isto porque a geometria e a posição são características dos espalhadores que, normalmente, os algoritmos não lidam com isso em separado.

            In many scenarios where algorithms can adequately reconstruct the geometry of scatterers, the error in detecting scatterer positions tends to be low. For example, in the study described in Section \ref{sec:results:shape:star}, the $\zeta_P$ indicator values obtained by LSM, OSM, BIM, CSI, and SOM were 0.049\%, 0.057\%, 0.25\%, 0.31\%, and 0.14\%, respectively. This occurs because geometry and position are scatterer characteristics that algorithms typically do not handle separately.
            
            % Uma exceção seria no caso de algoritmos que resolvem o problema de imageamento como um problema de otimização no qual as variáveis de decisão são a posição, os vértices (ou outras características de uma geometria predefinida) e o contraste dos espalhadores \cite{michalski2000electromagnetic,salucci2022learned,zardi2025physics}. Além disso, em cenários com espalhadores fortes, os métodos tradicionais geralmente não conseguem recuperar muito bem a geometria dos espalhadores, dada a maior dificuldade do problema. Nesses cenários, o estudo da detecção da posição pode ser mais relevante.

            An exception would be algorithms that solve the imaging problem as an optimization problem, where the decision variables are the position, vertices (or other characteristics of a predefined geometry), and contrast of the scatterers \cite{michalski2000electromagnetic,salucci2022learned,zardi2025physics}. Additionally, in scenarios involving strong scatterers with small size compared to the wavelength, traditional methods generally struggle to recover scatterer geometry effectively due to the increased problem hardness. In such scenarios, studying position detection becomes more relevant.

            % Levando isso em consideração, os experimentos dessa seção foram formulados considerando espalhadores fortes, nos quais os métodos tradicionais não conseguem recuperar adequadamente a geometria dos espalhadores. Com isso, o indicador de erro de posição $\zeta_P$ pode ser uma métrica relevante para comparar desempenho nesse tipo de cenário.

            Considering this, the experiments in this section were designed for scenarios involving strong scatterers with small size, where traditional methods cannot adequately recover scatterer geometry. Therefore, the position error indicator $\zeta_P$ serves as a relevant metric for comparing algorithm performance in such challenging scenarios. In addition, the DOI discretization for data synthesis was performed using a finer grid with $160 \times 160$ pixels.

            Considering this, the experiments in this section were designed for scenarios involving strong scatterers with small dimensions, where traditional methods cannot adequately recover scatterer geometry. Therefore, the position error indicator $\zeta_P$ serves as a relevant metric for comparing algorithm performance in such challenging scenarios.

            For all subsequent experiments, LSM was removed and an other method was introduced. Since the analytical solution for scattering from an infinite dielectric cylinder is well-established \cite{harrington2001time}, it becomes possible to determine the optimal circular approximation (defined by radius, position, and contrast) that best fits the scattered field of any single scatterer.

            To optimize these four parameters, we employ a two-stage approach \cite{virtanen2020SciPy}:
            \begin{enumerate}
                \item Global optimization: The Differential Evolution (DE) algorithm \cite{storn1997differential} with 50 generations, 15 individuals, crossover rate of 0.7, and mutation factor ranging from 0.5 to 1.
                \item Local refinement: The L-BFGS-B algorithm \cite{zhu1997algorithm} for improved accuracy.
            \end{enumerate}

            While DE is inherently stochastic and would typically require multiple executions for robust analysis, the problem structure and subsequent refinement step allow for single execution per instance based on previous experimental validation. This methodology is referred to as Circle Approximation (CA) throughout this work.

            It should be noted that \cite{michalski2000electromagnetic} employed a similar DE-based approach for circular fitting, though their work focused specifically on perfect electric conductor cylinders under cylindrical scatterer assumptions.

			\subsubsection{Single scatter}\label{sec:results:position:single}

                The first scenario for studying the position indicator utilized a star-shaped scatterer, similar to the experiments described in Sections \ref{sec:results:shape:star} and \ref{sec:results:shape:varying}. However, this configuration featured the scatterer positioned in the upper-right corner and rotated by 30 degrees, as illustrated in Fig. \ref{fig:position:single:ground_truth}. The scatterer contrast was set to 7, while the radius from the center to the farthest vertices was reduced to 0.15$\lambda_b$. Under these configurations, the DNL for this experiment reaches 1.7, which is slightly above the threshold of 1.

                % A Fig. \ref{fig:position:single:recons} mostra as imagens reconstruídas por cada algoritmo. Conforme esperado, uma vez que o contraste é alto e o tamanho do espalhador é pequeno, os métodos vão reconstruir pequenos borrões na região no espalhador. A maneira como esses borrões são reconstruídos é importante, visto que essa informação impacta na medição da posição do espalhador. Tendo isto em vista, o CSI foi quem teve a maior dificuldade na reconstrução, uma vez o contraste se concentrou em uma região muito pequena e que o nível de contraste foi bem baixo se comparado aos demais métodos quantitativos. Nota-se também que o CA foi quei mais se aproximou do contraste do espalhador.

                Fig. \ref{fig:position:single:recons} shows the images reconstructed by each algorithm. As expected, given the high contrast and small scatterer size, the methods reconstruct small blobs in the scatterer region. The way these blobs are reconstructed is relevant, as this information directly impacts the measurement of scatterer position. 

                In this regard, CSI experienced the greatest reconstruction difficulty, with contrast concentrated in a very small region and significantly lower contrast levels compared to the other quantitative methods. It is also notable that CA most closely approximated the actual scatterer contrast.

                \begin{figure*}[!htb]
                    \subfigure[Ground-Truth]{\includegraphics[height=0.55\columnwidth]{../experiments/position/single/figs/0.eps}\label{fig:position:single:ground_truth}}
                    \subfigure[OSM]{\includegraphics[height=0.55\columnwidth]{../experiments/position/single/figs/1.eps}}
                    \subfigure[BIM]{\includegraphics[height=0.55\columnwidth]{../experiments/position/single/figs/2.eps}} \\
                    \subfigure[CSI]{\includegraphics[height=0.55\columnwidth]{../experiments/position/single/figs/3.eps}}
                    \subfigure[SOM]{\includegraphics[height=0.55\columnwidth]{../experiments/position/single/figs/4.eps}}
                    \subfigure[CA]{\includegraphics[height=0.55\columnwidth]{../experiments/position/single/figs/5.eps}}
                    \caption{Position detection study, single scatterer: ground-truth and reconstructed images by each algorithm.}
                    \label{fig:position:single:recons}
                \end{figure*}

                Table \ref{tab:position:single:zeta_p} summarizes the results of the position error indicator for each algorithm. The results indicate that OSM, BIM, and CA achieved the best performance, while SOM performed similarly. The largest error was observed for CSI, which, due to its greater difficulty in quantifying the object's contrast, had more difficulty in capturing the object's position. Generally, the position detection errors for most algorithms were low. However, there may be applications where precision in position detection is critical, such as in tumor detection, for example.

                \begin{table}[!htb]
                    \centering
                    \renewcommand{\arraystretch}{1.5}
                    \caption{Position detection study, single scatterer: position error indicator $\zeta_P$ for each algorithm.}
                    \label{tab:position:single:zeta_p}
                    \begin{tabular}{cccccc}
                        Method & OSM & BIM & CSI & SOM & CA \\\hline
                        $\zeta_P$ (\%) & 0.08 & 0.04 & 4.68 & 0.53 & 0.04 \\
                    \end{tabular}   
                \end{table}
			
			\subsubsection{Multiple scatterers}\label{sec:results:position:multiple}
			
                % Embora o indicador proposto para mensurar o erro na posição tenha sido pensado para cenários com um único espalhador, ele também pode ser aplicado em cenários com múltiplos espalhadores. Nestes casos, o centróide do arranjo como um todo é o ponto de referência para a medição do erro na posição. Este tipo de investigação pode ser útil para aplicações como through-wall imaging, por exemplo.

                Although the proposed indicator for measuring position error was designed for scenarios with a single scatterer, it can also be applied in scenarios with multiple scatterers. In these cases, the centroid of the arrangement as a whole serves as the reference point for measuring position error. This type of investigation can be useful for applications such as through-wall imaging, for example.

                Based on this approach, this experiment considers three scatterers: the star from the previous experiment, a square, and a cross. The contrasts of the three scatterers were set to 4, 3.5, and 2.5, respectively. These values were chosen to be lower than in the previous experiment, since the presence of multiple scatterers inherently increases problem hardness due to mutual current induction between the scatterers.

                The square has a side length of 0.15$\lambda_b$, while the cross has dimensions of 0.3$\lambda_b$ × 0.3$\lambda_b$ with a thickness of 0.15$\lambda_b$. The scatterers were positioned such that their collective arrangement is located in the upper corner of the DOI, as illustrated in Fig. \ref{fig:position:multiple:ground_truth}. Under this configuration, the DNL for this experiment is 0.3.

                Fig. \ref{fig:position:multiple:recons} presents the reconstructions obtained by each algorithm for the multiple scatterer scenario. As expected in this small-size configuration, the algorithms produce blob-like structures that are generally larger than the actual object geometries. The CA method positioned its circular approximation close to the square's location. Although the quantitative methods could not accurately estimate the individual scatterer contrasts, they consistently assigned higher contrast values to the square region rather than the star region. This behavior suggests that the square's larger cross-sectional area compared to the star may have influenced the algorithms' reconstruction process.

                \begin{figure*}[!htb]
                    \subfigure[Ground-Truth]{\includegraphics[height=0.55\columnwidth]{../experiments/position/multiple/figs/0.eps}\label{fig:position:multiple:ground_truth}}
                    \subfigure[OSM]{\includegraphics[height=0.55\columnwidth]{../experiments/position/multiple/figs/1.eps}}
                    \subfigure[BIM]{\includegraphics[height=0.55\columnwidth]{../experiments/position/multiple/figs/2.eps}} \\
                    \subfigure[CSI]{\includegraphics[height=0.55\columnwidth]{../experiments/position/multiple/figs/3.eps}}
                    \subfigure[SOM]{\includegraphics[height=0.55\columnwidth]{../experiments/position/multiple/figs/4.eps}}
                    \subfigure[CA]{\includegraphics[height=0.55\columnwidth]{../experiments/position/multiple/figs/5.eps}}
                    \caption{Position detection study, multiple scatterer: ground-truth and reconstructed images by each algorithm.}
                    \label{fig:position:multiple:recons}
                \end{figure*}

                Table \ref{tab:position:multiple:zeta_p} presents the position error results for each algorithm. Interestingly, CSI achieved the best performance in this multi-scatterer scenario, despite its poor performance in the single scatterer case. This suggests that the algorithm's behavior may be different when dealing with multiple targets. BIM and SOM showed similar intermediate performance, while OSM and CA exhibited higher position errors, indicating greater difficulty in accurately localizing the overall scatterer arrangement.

                \begin{table}[!htb]
                    \centering
                    \renewcommand{\arraystretch}{1.5}
                    \caption{Position detection study, multiple scatterers: position error indicator $\zeta_P$ for each algorithm.}
                    \label{tab:position:multiple:zeta_p}
                    \begin{tabular}{cccccc}
                        Method & OSM & BIM & CSI & SOM & CA \\\hline
                        $\zeta_P$ (\%) & 10.24 & 8.30 & 4.86 & 8.27 & 12.85 \\
                    \end{tabular}   
                \end{table}
                
			\subsubsection{Average performance}\label{sec:results:position:benchmark}

                Similar to the shape recovery study, an average performance analysis was conducted to compare the position detection capabilities of OSM, BIM, CSI, SOM, and CA algorithms. This experiment employed 30 scatterer instances with configurations similar to those described in Section \ref{sec:results:position:single}. The scatterer contrast was fixed at 7, and the radius from the center to the farthest vertices was set to 0.15$\lambda_b$. Following the approach from Section \ref{sec:results:shape:average}, the scatterers were randomly generated as polygons with 4 to 15 vertices.

                The DNL distribution is shown in Fig. \ref{fig:position:average:dnl}. The values range from below 1 to above 4, with a median close to 2. The high contrast level and geometric variations contributed to the wide distribution of DNL values. However, as shown in Fig. \ref{fig:position:average:zeta_s}, the algorithm performance within each instance remained within a similar range. CSI clearly exhibited the worst performance, followed by SOM. The best performance in each instance varied between OSM and CA. These results are coherent with the findings from Section \ref{sec:results:position:single}.

                \begin{figure}[!htb]
                    \centering
                    \subfigure[]{\includegraphics[width=.5\columnwidth]{./experiments/position/average/figs/dnl.eps}\label{fig:position:average:dnl}} \\
                    \subfigure[]{\includegraphics[width=\columnwidth]{./experiments/position/average/figs/plot.eps}\label{fig:position:average:zeta_s}}
                    \caption{Position detection study, average performance: (a) Degree of Nonlinearity (DNL) for each scatterer; (b) position error indicator $\zeta_P$ for each algorithm in each instance.}
                    \label{fig:position:average}   
                \end{figure}

                % Embora a Fig. \ref{fig:position:average:zeta_s} sugira claramente que há uma diferença no desempenho médio dos algoritmos, foi aplicado novamente o RCBD para fornecer evidências estatísticas para isso. O CSI foi removido desta análise, uma vez que seu desempenho foi significativamente pior que os demais algoritmos. Após validar o pressuposto de normalidade, o valor-p obtido pelo RCBD foi menor que $10^{-30}$. Nas comparações todos-contra-todos feitas através de múltiplos testes T pareados com correção de Bonferroni, o valor-p foi menor que $10^{-6}$ para todos os pares de algoritmos, a hipótese nula de igualdade de desempenho médio só não foi rejeitada para o par OSM-CA. Ou seja, não foram encontradas evidências estatísticas de que OSM e CA possuem desempenhos médios diferentes, sugerindo então que ambos os métodos possuem desempenhos equivalentes. Os valores-p obtidos pelos testes são apresentados na Tabela \ref{tab:position:average:pvalue}.

                While Fig. \ref{fig:position:average:zeta_s} clearly suggests differences in average algorithm performance, RCBD was applied to provide statistical evidence. CSI was excluded from this analysis due to its significantly inferior performance compared to other algorithms. 

                After validating the normality assumption, the RCBD yielded a p-value less than $10^{-30}$, indicating significant differences among algorithms. Multiple paired T-tests with Bonferroni correction revealed p-values less than $10^{-6}$ for all algorithm pairs, except for the OSM-CA comparison. The null hypothesis of equal average performance was rejected for all pairs except OSM-CA, indicating that these two methods demonstrate statistically equivalent performance. The complete p-values are presented in Table \ref{tab:position:average:pvalue}.

                \begin{table*}
                    \centering
                    \renewcommand{\arraystretch}{1.5}
                    \caption{Position detection study, average performance: $p$-values for the paired T-Test with Bonferroni correction.}
                    \label{tab:position:average:pvalue}
                    \begin{tabular}{ccccccc}
                        Pairs & OSM-BIM & OSM-SOM & OSM-CA & BIM-SOM & BIM-CA & SOM-CA \\\hline
                        $p$-value & $<10^{-9}$ & $<10^{-13}$ & 0.95 & $<10^{-6}$ & $<10^{-10}$ & $<10^{-17}$ \\  
                    \end{tabular}
                \end{table*}

                Fig. \ref{fig:position:average:confidence_intervals} shows the 95\% confidence intervals for the mean paired differences of the position error indicator $\zeta_P$ for each algorithm pair. The results demonstrate that OSM and CA achieve superior performance compared to BIM and SOM, while BIM outperforms SOM. However, the mean differences do not exceed 1\% in any case, indicating that although statistically significant differences exist, these differences would only be practically relevant in applications requiring high precision in position detection.

                \begin{figure}
                    \centering
                    \includegraphics[width=.8\columnwidth]{./experiments/position/average/figs/confidence_intervals.eps}
                    \caption{Position detection study, average performance: Confidence intervals (95\%) for the position error indicator $\zeta_P$ for each pair of algorithms.}
                    \label{fig:position:average:confidence_intervals}
                \end{figure}

        \subsection{Breast Phantom Study}\label{sec:results:breast}

            Finally, a case study using a realistic breast phantom demonstrates the application of the proposed indicators in clinical scenarios. This phantom was obtained from the UWCEM Numerical Breast Phantom Repository \cite{burfeindt2012mri} and corresponds to the Very Dense type (ACR class 4). 

            Since the repository provides three-dimensional phantoms while our scattering model is two-dimensional, a representative cross-section focusing on the breast interior was extracted, as shown in Fig. \ref{fig:breast:phantom:ground_truth}. The resulting image contains 328 $\times$ 328 pixels with a spatial resolution of 0.5 mm $\times$ 0.5 mm per pixel. 

            The phantom includes four tissue types: fatty, transitional, fibroglandular, and skin. The breast is immersed in a homogeneous background medium with relative permittivity $\epsilon_{rb} = 10$ and conductivity $\sigma = 0$ S/m. The operating frequency was set to 1 GHz, with electrical properties of breast tissues determined using the Debye model.

            The experimental configuration follows the previous studies: 80 incident sources, 80 measurement points, and 5\% noise level. Under these conditions, the DNL for this experiment is 0.05. Since the interior of the breast is a complex structure, keeping a low DNL value is an important strategy to facilitate the reconstruction.

    \section{Conclusion}\label{sec:conclusion}
        A conclusion section is not required. Although a conclusion may review the main points of the paper, do not replicate the abstract as the conclusion. A  conclusion might elaborate on the importance of the work or suggest applications and extensions. \cite{chen2018computational}
    
    \appendices
    
    \section*{Acknowledgment}
    
        Use the singular heading even if you have many acknowledgments. In most  cases, sponsor and financial support acknowledgments are placed in the  unnumbered footnote on the first page, not here.
    
    \bibliographystyle{IEEEtran}
    \bibliography{mybib}
    
    % \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{a1.png}}]
    %     {First A. Author} (M'76--SM'81--F'87) and all authors may include biographies. Biographies are often not included in conference-related papers. This author became a Member (M) of IEEE in 1976, a Senior Member (SM) in 1981, and a Fellow (F) in 1987. The first paragraph may contain a place and/or date of birth (list place, then date). Next, the author's educational background is listed. The degrees should be listed with type of degree in what field, which institution, city, state, and country, and year the degree was earned. The author's major field of study should be lower-cased. 
        
    %     The second paragraph uses the pronoun of the person (he or she) and not the author's last name. It lists military and work experience, including summer and fellowship jobs. Job titles are capitalized. The current job must have a  location; previous positions may be listed without one. Information concerning previous publications may be included. Try not to list more than three books or published articles. The format for listing publishers of a book within the biography is: title of book (publisher name, year) similar to a reference. Current and previous research interests end the paragraph. The third paragraph begins with the author's title and last name (e.g., Dr.\ Smith, Prof.\ Jones, Mr.\ Kajor, Ms.\ Hunter). List any memberships in professional societies other than the IEEE. Finally, list any awards and work for IEEE committees and publications. If a photograph is provided, it should be of good quality, and professional-looking. Following are two examples of an author's biography.
    % \end{IEEEbiography}
    
    % \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{a2.png}}]
    %     {Second B. Author} was born in Greenwich Village, New York, NY, USA in 1977. He received the B.S. and M.S. degrees in aerospace engineering from the University of Virginia, Charlottesville, in 2001 and the Ph.D. degree in mechanical engineering from Drexel University, Philadelphia, PA, in 2008.
        
    %     From 2001 to 2004, he was a Research Assistant with the Princeton Plasma Physics Laboratory. Since 2009, he has been an Assistant Professor with the Mechanical Engineering Department, Texas A{\&}M University, College Station. He is the author of three books, more than 150 articles, and more than 70 inventions. His research interests include high-pressure and high-density nonthermal plasma discharge processes and applications, microscale plasma discharges, discharges in liquids, spectroscopic diagnostics, plasma propulsion, and innovation plasma applications. He is an Associate Editor of the journal \emph{Earth, Moon, Planets}, and holds two patents. 
        
    %     Dr. Author was a recipient of the International Association of Geomagnetism and Aeronomy Young Scientist Award for Excellence in 2008, and the IEEE Electromagnetic Compatibility Society Best Symposium Paper Award in 2011. 
    % \end{IEEEbiography}
    
    % \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{a3.png}}]
    %     {Third C. Author, Jr.} (M'87) received the B.S. degree in mechanical engineering from National Chung Cheng University, Chiayi, Taiwan, in 2004 and the M.S. degree in mechanical engineering from National Tsing Hua University, Hsinchu, Taiwan, in 2006. He is currently pursuing the Ph.D. degree in mechanical engineering at Texas A{\&}M University, College Station, TX, USA.
        
    %     From 2008 to 2009, he was a Research Assistant with the Institute of Physics, Academia Sinica, Tapei, Taiwan. His research interest includes the development of surface processing and biological/medical treatment techniques using nonthermal atmospheric pressure plasmas, fundamental study of plasma sources, and fabrication of micro- or nanostructured surfaces. 
        
    %     Mr. Author's awards and honors include the Frew Fellowship (Australian Academy of Science), the I. I. Rabi Prize (APS), the European Frequency and Time Forum Award, the Carl Zeiss Research Award, the William F. Meggers Award and the Adolph Lomb Medal (OSA).
    % \end{IEEEbiography}

\end{document}

% Figure example.
\begin{figure}[!t]
    \centerline{\includegraphics[width=\columnwidth]{fig1.png}}
    \caption{Magnetization as a function of applied field. It is good practice to explain the significance of the figure in the caption.}
    \label{example}
\end{figure}

% Table example.
\begin{table}
    \caption{Units for Magnetic Properties}
    \label{table}
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{|p{25pt}|p{75pt}|p{115pt}|}
        \hline
        Symbol& Quantity& Conversion from Gaussian and \par CGS EMU to SI $^{\mathrm{a}}$ \\\hline
        $\Phi $& magnetic flux& 1 Mx $\to  10^{-8}$ Wb $= 10^{-8}$ V$\cdot $s \\
        $B$& magnetic flux density, \par magnetic induction& 1 G $\to  10^{-4}$ T $= 10^{-4}$ Wb/m$^{2}$ \\
        $H$& magnetic field strength& 1 Oe $\to  10^{3}/(4\pi )$ A/m \\
        $m$& magnetic moment& 1 erg/G $=$ 1 emu \par $\to 10^{-3}$ A$\cdot $m$^{2} = 10^{-3}$ J/T \\
        $M$& magnetization& 1 erg/(G$\cdot $cm$^{3}) =$ 1 emu/cm$^{3}$ \par $\to 10^{3}$ A/m \\
        4$\pi M$& magnetization& 1 G $\to  10^{3}/(4\pi )$ A/m \\
        $\sigma $& specific magnetization& 1 erg/(G$\cdot $g) $=$ 1 emu/g $\to $ 1 A$\cdot $m$^{2}$/kg \\
        $j$& magnetic dipole \par moment& 1 erg/G $=$ 1 emu \par $\to 4\pi \times  10^{-10}$ Wb$\cdot $m \\
        $J$& magnetic polarization& 1 erg/(G$\cdot $cm$^{3}) =$ 1 emu/cm$^{3}$ \par $\to 4\pi \times  10^{-4}$ T \\
        $\chi , \kappa $& susceptibility& 1 $\to  4\pi $ \\
        $\chi_{\rho }$& mass susceptibility& 1 cm$^{3}$/g $\to  4\pi \times  10^{-3}$ m$^{3}$/kg \\
        $\mu $& permeability& 1 $\to  4\pi \times  10^{-7}$ H/m \par $= 4\pi \times  10^{-7}$ Wb/(A$\cdot $m) \\
        $\mu_{r}$& relative permeability& $\mu \to \mu_{r}$ \\
        $w, W$& energy density& 1 erg/cm$^{3} \to  10^{-1}$ J/m$^{3}$ \\
        $N, D$& demagnetizing factor& 1 $\to  1/(4\pi )$ \\\hline
        \multicolumn{3}{p{251pt}}{Vertical lines are optional in tables. Statements that serve as captions for the entire table do not need footnote letters. }\\
        \multicolumn{3}{p{251pt}}{$^{\mathrm{a}}$Gaussian units are the same as cg emu for magnetostatics; Mx $=$ maxwell, G $=$ gauss, Oe $=$ oersted; Wb $=$ weber, V $=$ volt, s $=$ second, T $=$ tesla, m $=$ meter, A $=$ ampere, J $=$ joule, kg $=$ kilogram, H $=$ henry.}
    \end{tabular}
    \label{tab1}
\end{table}

